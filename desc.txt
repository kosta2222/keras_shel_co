Более подробное описание:
//
Постоянно сохраняет в:
график истории в ./graphic/train_graphic.png
топологию модели в model_json.json
веса модели в wei.h5
//
stop=0  // останавливает выполнение
push_i = 1 // ложит целое
push_fl = 2 // ложит дрпобное
push_str = 3 // ложит строку
cr_nn_ = 4  // создает модель
predict = 5 // предсказание
evalu_= 6  // оценка
determe_X_Y = 7 // выявить X и Y
sav_model = 8 // сохранить топологию модели
load_model_wei = 9 // загрузить веса модели и топологию модели
get_weis = 10 // записать веса в список из модели (для обратной-reverse сети)
make_net = 11 // создать модель
k_summary = 12 // суммарность модели
compile_net = 13 // скомпилировать модель
fit_net = 14 // обучить модель
make_net_load_wei = 15 //
plot_train = 16 // построить график обучения
get_mult_class_matr = 27 // считать картинки и папок(классов) выдать X и Y
cr_sav_model_wei_best_callback = 18 // сохранение весов с лучшими параметрами
sav_model_wei = 19 // сохранить веса модели
cr_callback_wi_loss_treshold_and_acc_shure = 20 // колбек с прерыванием обучения с порогоми
push_obj = 21 // ложит обьект список или матрицу
nparray = 22 // переводит что в стековой памяти в numpy массив
make_net_on_contrary = 23 // создать сеть наоборот используя веса и топологию 'прямой' сети
//
Картинки для обучения в ./train_ann/train
Картинки для спрашивания ./train_ann/ask
//
Создать последовательную сеть(Sequential) с биасами с топологией (2,3,1),с активациями
-relu и сигмоид, с постоянной инициализацией так:
my_init=My_const_init(9)
ke_init=("glorot_uniform",my_init)
make_net,('S', ('D','D'), (2,3,1),('r','s'), ('use_bias_1','use_bias_1','use_bias_1'),ke_init[0])
...
stop
Мои отображения на стандартные функции активации keras:
acts_di={'s':'sigmoid','r':'relu','t':'tanh','S':'softmax'}
Как сделать из данной сети (ее весов) сеть наоборот и спросить ее с конца(реализовано если сеть с биасами):
# Сеть от логического И наоборот
p21=    (load_model_wei,
         get_weis,
         make_net_on_contrary,('S',('D','D'),(2, 3, 1),('r','s'),('use_bias_1', 'use_bias_1'),ke_init[0]),
         compile_net, (opt, compile_pars[1], compile_pars[2]),
         push_obj,[[1]], # Y
         nparray,
         push_obj,[[1]], # X
         nparray,
         determe_X_Y,
         predict,
         stop)
//
Если использовать управляющий код
cr_callback_wi_loss_treshold_and_acc_shure = 20 // колбек с прерыванием обучения с порогами
то последующими кодами не получится построить график обучения, программа завершается с сохранением
топологии модели и ее весов.
// loger задается так:
loger, date=get_logger("debug","log.txt",__name__,'w')
из модуля util
"debug" или "release" опции вывода в файл параметра "log.txt",
"w" или "a"(w по умолчанию)-w перезапись,a - добавка
__name__ - имя данного модуля
date - сегодняшняя дата и время

// Результаты работы с такой сетью
Обратная-reverse сеть получилась,она стала как бы архиватором, я обучал ее на 5 нарисованных
тюльпанах(градации серого) они были еще перевернутыми они были 100x100пикселей.В конце было 2 выхода
с softmax активацией и все ряды изображения приводились к [0,1], потом я развернул сеть и с помощью
PIL получил картинку(когда подал на predict [[0,1]]) все эти тюлпаны на одном холсте.

Вот с бинарной классификацией с sigmoid на конце не получилось,может обучающий набор маленький,было
5 кругов (градации серого) по 32x32пикселя.acc была==1 loss было 0.01 и 0.0001 но это не помогло
после обучения отличить тот же круг от треугольника-для них писала все как [[1.],[1.]]
